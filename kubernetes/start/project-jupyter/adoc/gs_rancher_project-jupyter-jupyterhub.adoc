:docinfo:
include::./common_docinfo_vars.adoc[]

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// GENERAL COMMENTS
//   - See the SUSE TRD Contributors Guide for detailed guidance:
//     https://documentation.suse.com/trd/contributors/single-html/suse-trd_contrib-guide/
//   - Keep in mind that this is a "getting started" guide.
//   - Write to the audience you are trying to reach.
//   - Add or remove sections and subsections as needed.
// 
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =


// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// DOCUMENT ATTRIBUTES AND VARIABLES
//
// NOTES:
// 1. Define and use document attributes or variables in this file.
// 2. Update the docinfo.xml file if needed.
//
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =


// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// ORGANIZATION
//   Do NOT modify this section.
// -
:trd: Technical Reference Documentation
:type: Getting Started
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -


// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// DOCUMENT REVISION DATE
//-
:revision-date: YYYY-MM-DD
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -


// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// DOCUMENT TITLE AND SUBTITLE
//   Provide concise but descriptive title and subtitle.
//   title - (<75 characters) concisely identify the guide.
//           (e.g.: "Kubeflow with Rancher")
//   subititle - (<75 characters) expound on the title.
//               (e.g., "Deploying Kubeflow onto an RKE2 Cluster with Rancher")
// -
:title: (<75 characters) Your Guide Title
:subtitle: (<75 characters) Your Guide Subtitle
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -


// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// TECHNICAL COMPONENTS
//   Identify the technical components featured in the guide.
//   Use variables to store information about the solution components.
//   This makes it much easier to update the document for future
//   version upgrades, URL changes, etc.
//
//   Variable names follow a simple pattern:
//   - for SUSE components: scompX[-MODIFIER]
//   - for partner/project components: pcompX[-MODIFIER]
//   where:
//   - X is an integer starting from 1 for the primary component,
//     2 for a secondary component, and so on.
//   - -MODIFIER is text that identifies the stored information.
//   Some -MODIFIER values include:
//   - EMPTY: If the modifier is missing, the variable contains
//     a short name for the component (e.g., 'SLES', 'Rancher Prime')
//   - -full: Long name of the component
//     (e.g., 'SUSE Linux Enterprise Server', 'Rancher Prime by SUSE')
//   - -provider: Name of company or project providing the component
//     (e.g., 'SUSE', 'HPE', 'Kubeflow', 'Veeam')
//   - -version: Relevant product version (e.g., '15', '15SP5', '2.7')
//     or versions (e.g., '15SP4, 15SP5', '15SP3+', '2.6+', '2.x')
//   - -website: Product website (e.g., https://www.suse.com/products/server/)
//   - -docs: Product documentation (e.g., https://documentation.suse.com/sles/)
//
//   You can create additional modifiers as needed.
//  
// -
// SUSE Components
// -

:rke2: RKE2
:rke2-full: Rancher Kubernetes Engine 2
:rke2-url: https://www.suse.com/products/rancher-kubernetes-engine/
:rke2-support-matrix: https://www.suse.com/suse-rke2/support-matrix/all-supported-versions/
:rke2-docs: https://docs.rke2.io/
:rke2-version: v1.27.8+rke2r1

:rancher: Rancher
:rancher-full: Rancher by SUSE
:rancher-url: https://www.suse.com/solutions/enterprise-container-management/#rancher-product
:rancher-docs: https://ranchermanager.docs.rancher.com/
:rancher-docs-rke2: {rancher-docs}pages-for-subheaders/launch-kubernetes-with-rancher#rke2
:rancher-kubectl-url: https://www.suse.com/c/rancher_blog/how-to-manage-kubernetes-with-kubectl/
:rancher-kubectl-shell: Kubectl Shell
:rancher-kubectl-shell-variables-file: /tmp/variables

:longhorn: Longhorn
:longhorn-version: 103.2.1+up1.5.3
:longhorn-url: https://longhorn.io/
:longhorn-docs: https://longhorn.io/docs/latest
:longhorn-installation-doc: https://ranchermanager.docs.rancher.com/integrations-in-rancher/longhorn/overview#installing-longhorn-with-rancher
:longhorn-env-check-script-doc: https://longhorn.io/docs/latest/deploy/install/#using-the-environment-check-script


:harvester: Harvester
:harvester-url: https://www.rancher.com/index.php/products/harvester
:harvester-version: v1.3
:harvester-docs: https://docs.harvesterhci.io/

:harvester-upload-image-url: https://docs.harvesterhci.io/v1.3/upload-image
:harvester-create-rke2-cluster-url: https://docs.harvesterhci.io/v1.3/rancher/node/rke2-cluster/#create-rke2-kubernetes-cluster
:harevester-cloud-configuration-url: https://docs.harvesterhci.io/v1.3/vm/index#cloud-configuration
:harvester-namespace: default
:harvester-cloud-config-template-name: rke2-ready
:virtualization-management: Virtualization Management
:cluster-management: Cluster Management

:sles: SLES
:sles-full: SUSE Linux Enterprise Server
:sles-minimal: SLES Minimal Cloud
:sles-version: 15
:sles-sp: 5
:sles-sp-full: SP{sles-sp}
:sles-url: https://www.suse.com/products/server/
:sles-docs: https://documentation.suse.com/sles/{sles-version}-{sles-sp-full}/
:sles-docs-modules: {sles-docs}single-html/SLES-modules/#sec-modules-install
:sles-minimal-download-url: https://www.suse.com/download/sles/
:sles-minimal-file-name: SLES15-SP5-Minimal-VM.x86_64-Cloud-GM.qcow2
:sles-minimal-saved-name: sles-{sles-version}-{sles-sp}-minimal
// -
// Partner/Project Components
// -
:jupyterhub-provider: Project Jupyter
:jupyterhub: JupyterHub
:jupyterhub-version: 4.0.2
:jupyterhub-website: https://jupyter.org/hub
:jupyterhub-docs: https://jupyterhub.readthedocs.io/en/latest/
:jupyterhub-installation-doc: https://z2jh.jupyter.org/en/stable/jupyterhub/installation.html
:jupyterhub-auth-doc: https://z2jh.jupyter.org/en/stable/administrator/authentication.html
:jupyterhub-kubernetes-customizations-doc: https://z2jh.jupyter.org/en/stable/jupyterhub/customization.html

:jupyterhub-deployed-name: jupyterhub
:jupyterhub-namespace: jupyterhub

:jupyterlab-provider: Project Jupyter
:jupyterlab: JupyterLab
:jupyterlab-version: 4.0.9
:jupyterlab-website: https://jupyter.org/
:jupyterlab-docs: https://jupyterlab.readthedocs.io/en/latest/
:juypter-notebooks-github-url: https://github.com/jupyter/notebook.git
:jupyter-notebooks-running-code-filename: Running Code.ipynb
:github-name: GitHub


:metallb: MetalLB
:metallb-url: https://metallb.org/
:metallb-docs: https://metallb.org/concepts/
:metallb-version: 0.13.12
:metallb-ipaddress-config-url: https://metallb.org/configuration/#defining-the-ips-to-assign-to-the-load-balancer-services
:metallb-namespace: metallb-system

:k8s: Kubernetes
:k8s-version: 1.27
:harvester-cloud-creds-url: https://docs.harvesterhci.io/v1.3/rancher/node/rke2-cluster#create-your-cloud-credentials

:lets-encrypt: Let's Encrypt
:lets-encrypt-base-staging-url: https://acme-staging-v02
:lets-encrypt-full-staging-url: https://acme-staging-v02.api.letsencrypt.org/directory
:lets-encrypt-base-prod-url: https://acme-v02
:lets-encrypt-full-prod-url: https://acme-v02.api.letsencrypt.org/directory
:lets-encrypt-dns-providers-url: https://community.letsencrypt.org/t/dns-providers-who-easily-integrate-with-lets-encrypt-dns-validation/86438
:public-domain-name: susealliances.com
:public-fqdn: {jupyterhub-deployed-name}.{public-domain-name}

:certbot-name: certbot
:certbot-package-name: python3-{certbot-name}
:certbot-dns-plugin-url: https://eff-certbot.readthedocs.io/en/stable/using.html#dns-plugins

:cert-manager: cert-manager
:cert-manager-docs: https://cert-manager.io/docs/
:cert-manager-dns01-rout53-doc: https://cert-manager.io/docs/configuration/acme/dns01/route53/
:cert-manager-version: v1.13.3
:cert-manager-crds-url: https://github.com/cert-manager/cert-manager/releases/download/{cert-manager-version}/cert-manager.crds.yaml

:aws-name-full: Amazon Web Services
:route53-name: Amazon Route 53


// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -


// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// DOCUMENT DESCRIPTIONS
//   Describe the guide by its purpose, use case, value.
//
// usecase: (<55 characters) key words or phrases that identify the
//          use case of this guide
//          (e.g.: "Database-as-a-Service", "edge analytics in healthcare",
//                 "Kubernetes-native object storage")
// description: (<150 characters) brief description of what this guide
//              provides (e.g.: "Deploy Kubeflow with Rancher Primer")
// description-short: (<55 characters) condensed description suitable for
//                    social media (e.g.: "Kubeflow with Rancher")
// executive-summary: (<300 characters) brief summary of the guide that
//                    appears near the beginning of the rendered document
//                    (e.g.: "Kubeflow simplifies deployment of machine
//                            learning (ML) workflows on Kubernetes clusters.
//                            This document provides step-by-step guidance
//                            for deploying Kubeflow on an RKE2 cluster with
//                            Rancher."
//
// -

:title: Deploy {jupyterhub} with {rancher-full}

:subtitle: Deploying {jupyterhub} with {rancher} using {longhorn} for highly available storage, {metallb} for load balancing, and {harvester} for compute virtualization.

:usecase: provide access to {jupyterlab} to teams of Developers and Data Scientists

:description: Deploy {jupyterhub} on {rke2} with {rancher-full}, {longhorn}, and {harvester}.

:description-short: Deploy {jupyterhub} with {rancher-full}

:executive-summary: Enable Developers and Data Scientists to take full advantage of {jupyterlab} from any device that provides a modern web browser.

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -


// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// CONTRIBUTORS
//   Specify information about authors, editors, and other
//   contributors here.
//   Follow the pattern to provide fist name, surname, job title,
//   and organization name for each contributor.
//   NOTE: To list additional authors or others on the cover page,
//         you must edit the docinfo.xml file as well.
// -
:author1_firstname: Alex
:author1_surname: Arnoldy
:author1_jobtitle: Global Alliance Solutions Architect
:author1_orgname: SUSE

:editor1_firstname: Terry
:editor1_surname: Smith
:editor1_jobtitle: Director of Global Partner Solutions
:editor1_orgname: SUSE

//:author2-firstname: first (given) name
//:contrib1-firstname: first (given) name
//:editor1-firstname: first (given) name
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -


// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// MISCELLANEOUS
//   Define any additional variables here for use within the document.
// -


// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -


= {title}: {subtitle}



== Introduction

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Provide a motivation for the document in 2-4 sentences to identify:
//   - what the document is about
//   - why it may be of interest to the reader
//   - what products are being highlighted
// Include an approved SUSE | Partner logo lock-up if possible
// Include any additional, relevant details
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =


{jupyterlab} provides a flexible, interactive environment where you develop code, explore data, and build visualizations, all within a single interface. 
{jupyterlab} is perfect for individual research, learning, and prototyping. 
However, {jupyterlab} runs as a local application, requiring each user's time and knowledge to install and maintain it.

{jupyterhub} provides all of the value of {jupyterlab}, from a centrally maintained {k8s} cluster. 
This removes the burden of installation and maintenance for the {jupyterlab} users, allowing them to get the full value of the tool from any device that provides a web browser. 
In addition {jupyterhub} fuels collaboration with the ability to work in individual or shared development environments, a powerful capability that isn't available to individual {jupyterlab} installations.  
This guide shows you how to unlock the full potential of {jupyterhub} by deploying through Rancher by SUSE, enabling scalable deployments, centralized management, and enhanced security. 



=== Scope

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Specify what this guide covers in no more than 2 sentences.
//   E.g., "You will learn how to ..."
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =


In this guide we will cover:

* creating a highly-available {rke2} cluster using {rancher} and {harvester}

* installing {longhorn} with {rancher}

* installing and configuring {metallb} with {rancher}

* installing {jupyterhub} with static authentication for a handful of users

* creating a sample workload in {jupyterhub} that demonstrates both private and collaborative workspaces

=== Audience

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Identify for whom this document is intended, perhaps with:
//   - topics of interests (e.g., machine learning, security, etc.)
//   - job roles (e.g., developer, administrator, platform architect, etc.)
//   - required skills
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

This guide is intended for high technology professionals (including {k8s} administrators, proficient MLOps practitioners, and application developers) seeking to unlock the full potential of their collaborative data science and development environment.

To be successful, you need a foundational understanding of {k8s}, {rancher}, and {jupyterlab}.


=== Acknowledgements

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Acknowledge contributors not listed as authors.
// Something like:
//   The authors wish to acknowledge the contributions of the following
//   individuals:
//   * {contrib1-firstname} {contrib1-surname}, {contrib1-jobtitle}, {contrib1-orgname}
//   * {contrib2-firstname} {contrib2-surname}, {contrib2-jobtitle}, {contrib2-orgname}
// NOTE: If there are none, comment out this section.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

The author wants to acknowledge contributions to this guide by:

* {author1_firstname} {author1_surname}, {author1_orgname}, {author1_jobtitle}
//* {contrib2_firstname} {contrib2_surname}, {contrib2_orgname}, {contrib2_jobtitle}
* {editor1_firstname} {editor1_surname}, {editor1_orgname}, {editor1_jobtitle}



== Prerequisites

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Identify minimum requirements (prerequisites) the reader
// would need in order to follow the steps of this guide.
// - Use an unordered list.
// - Link to existing resources whenever possible.
// - Keep this section brief but elaborate as needed.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

Before you embark on the procedures outlined in this guide, ensure that your environment is ready with the resources listed here.

.*{rancher} installation*
//
+
{rancher} can be running in a public cloud provider or on-premises, but needs needs to have access to the infrastructure provider for the {rke2} cluster.

. A {harvester} cluster 
//
+
{harvester} will be the infrastructure provider for the {rke2-full} cluster. {rancher} must be configured with cloud credentials with full access the {harvester}. 
See {harvester-cloud-creds-url}[for more information].
//
+
[NOTE]
====
We have selected the {harvester} infrastructure provider because of its seamless integration with {rancher}, and its ability to provide a low-cost, cloud-like Infrastructure as a Service experience for its users. 
If you don't have a {harvester} cluster readily available, {rke2} can be deployed on another infrastructure provider, or a hosted {k8s} can be used.
====
+

*At least one dedicated IP address*
//
+
You will need to dedicated one IP address to the {jupyterhub} service.
Additional IP addresses can be allocated for other functions on this {k8s} cluster. 
All IP addresses will be managed by the {metallb} load balancer.
+
[TIP]
====
In this design, {jupyterhub} is behind a corporate firewall/NAT router and uses a non-routable IP address. 
This means our clients will need access to the public internet to resolve the {jupyterhub} fully qualified domain name through {route53-name} AND VPN access to connect to our {jupyterhub} service's IP address.
====
+

*TLS certificate and private key*
//
+
A TLS certificate and private key from a publicly recognized provider are required to provide full HTTPS security when connecting to this deployment of {jupyterhub}. 
This document will include guidance for obtaining these items from {lets-encrypt}. 
//{lets-encrypt} will require that you provide the credentials to make changes to a publicly resolvable DNS domain, therefore you must also possess a publicly registered DNS domain as well as the API credentials to add and remove records from it. 
+
//[NOTE]
//====
//While it is not recommended, this deployment can be completed with HTTP communication, sacrificing security for simplicity. 
//This should never be done in production, or otherwise valuable, environments.
//====
+
[IMPORTANT]
====
It is out of the scope of this document to cover the process for providing TLS certificates from a private CA.
====
+
. A publicly resolvable DNS domain name

This guide will focus on using {route53-name} to provide a Fully Qualified Domain Name (FQDN) for the {jupyterhub} service. 
You can easily obtain a domain name from most public DNS providers, for a small fee. 
Let's Encrypt will require that you provide API credentials that can add and remove records from your DNS domain to verify your ownership of it. 
See this link:{lets-encrypt-dns-providers-url}[blog post] for a general list of supported DNS providers.
+
[CAUTION]
====
It is HIGHLY recommended that the API credentials provided possess no additional capabilities beyond adding/removing records from the domain.
====
////
.*Required software is installed on each node of the {rke2} cluster*
//
+
[NOTE]
====
This requirement can only be completed after the {rke2} cluster has been deployed. 
After deploying the {rke2} cluster, you can return here to complete this step.
====
+
Longhorn leverages several open source software projects, including open-iscsi and nfs-client, for core functionality. 
You must ensure that each node in the {rke2} cluster meets link:https://longhorn.io/docs/1.5.3/deploy/install/#installation-requirements[these requirements].
+
Most of the required software can be installed on {sles-full} with this command:
+
[source, console]
----
sudo zypper in coreutils jq nfs-client util-linux open-iscsi
----
////

== Set up your environment


== Overview

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Provide an overview of the solution and the processes detailed
// in this guide.
// - Identify components
// - Describe how the components fit together
// - Leverage diagrams as appropriate, including (but not limited to):
//   - component architecture
//   - data flow diagram
//   - workflow diagram
// - Summarize the major steps the reader will perform
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

This Getting Started guide will help you build a production quality JupyterHub environment running on an {rke2} {k8s} cluster deployed on virtual machines provided by the {harvester} hypervisor. 
We'll navigate the journey step-by-step, covering:

* Deploying {rke2-full} on {harvester}: You'll leverage {rancher-full} for {k8s} application and cluster management. 

* Configuring {longhorn} for for highly available, persistent storage.

* Configuring {metallb}: This versatile load balancer provides virtual IP addresses that ensures traffic to your {jupyterlab} server instances isn't interrupted, even after a node failure.

* Installing {jupyterhub}: Using its Helm chart, we'll seamlessly integrate {jupyterhub} within your RKE2 cluster, equipping you with a dynamic platform for interactive data analysis and collaboration.

== Configure {harvester} to deploy properly configured virtual machines

Download {sles-minimal-file-name} image from link:{sles-minimal-download-url}[this URL] (You may need to log in or create a free account).
+
Following link:{harvester-upload-image-url}[these instructions], upload the {sles-minimal} image to {harvester}
+
[NOTE] 
====
In uploading the {sles-minimal} image to {harvester}, we used these parameters:

* Namespace: `{harvester-namespace}`

* Name: `{sles-minimal-saved-name}`
====

Create a `Cloud Config Template` based on link:{harevester-cloud-configuration-url}[these instructions].
+
[NOTE]
====
For this procedure, we have named this Cloud Configuration Template `{harvester-cloud-config-template-name}`
====
+
[NOTE]
====
If you prefer, configure a more secure password for the root and sles users.
====
+
[IMPORTANT]
====
Uncomment and update ONLY ONE the `SUSEConnect` commands below to register the VM operating systems either via an RMT server URL or with a subscription code (i.e. regcode). 
Be sure to remove the other command.
====
+
[source, console, subs="attributes+"]
----
### cloud-init
#cloud-config
chpasswd:
  list: |
    root:SUSE
    sles:SUSE
  expire: false
runcmd:
#  - SUSEConnect --url http://rmt.example.com
#  - SUSEConnect --regcode MyActivationCode --email MyEmailAddress
  - zypper -n in -t pattern apparmor
  - zypper -n in iptables coreutils jq nfs-client util-linux open-iscsi
  - zypper -n up
  - zypper in --force-resolution --no-confirm --force kernel-default
  - systemctl enable iscsid
  - systemctl start iscsid
----

== Deploy {rke2-full} on {harvester}

Follow the instructions at link:{harvester-create-rke2-cluster-url}[this URL] to create an {rke2} cluster using {rancher} and {harvester}.

[NOTE]
====
In creating the {rke2} cluster, we have used these specific parameters:

*Machine Pools pane*

* Cluster Name: `jupyterhub`
* Machine Count: `3`
* Roles: `etcd`, `Control Plane`, and `Worker`
* CPUs (per node): `8` (4 can be used for smaller deployments)
* Memory (per node): `16` (8 can be used for smaller deployments)
* Namespace: `{harvester-namespace}`
* SSH User: `sles`
* Image: {sles-minimal-saved-name}
* Network: (Select a network to which all {jupyterlab} clients can connect)

*Click "Show Advanced"* 

*User Data:* 

* Uncheck the `Install guest agent` checkbox.
* User Data Template: `{harvester-cloud-config-template-name}`
** Ensure the cloud configuration template data shown is the same as you configured earlier
* Check the `Install guest agent` checkbox.

*Cluster Configuration pane*

*Basics tab*
* Kubernetes Version: {rke2-version}
====

[TIP]
====
When selecting the `User Data` template, some browsers won't pull in the correct data if the `Install guest agent` checkbox is selected. To ensure a consistent experience, you can first unchecked that box, selected the correct user data, then checked that box again.
====

After the {rke2} cluster has been deployed, its status will become `Active`. 
This can take five to ten minutes, depending on the performance of the underlying infrastructure. 

Please continue this procedure only after the cluster status shows `Active`.

== Reboot the {rke2} cluster nodes

After the {rke2} cluster has been deployed, each of the VMs in the cluster will need to be rebooted. 
This is required to activate the "kernel-default" kernel. 
{longhorn} cannot operate with the minimal "kernel-default-base" kernel.

. In the {rancher} UI, select {virtualization-management}, select the {harvester} cluster, then select `Virtual Machines`
. Select the three dots to the right of the first virtual machine in {jupyterhub} cluster, then select `Restart`
. Wait for the virtual machine to show `Active` again, and for it to show its IP address, then repeat with the next virtual machine in the cluster
. After the final virtual machine has completed its restart return to {cluster-management} and verify the {jupyterhub} cluster is `Active`

== Create the variables ConfigMap
Certain information is needed multiple times throughout the build process and even deployment process.
This information can be stored in environment variables and recalled as needed, helping you to streamline commands, avoid errors, and maintain consistency.

. Create the `variables` configmap in the `cattle-system` namespace:
.. In the {rancher} UI, select the Home icon from the menu on the left
.. Select the {jupyterhub} cluster
.. At the top center of the screen, change `Only User Namespaces` to `All Namespaces`
.. Select `Storage`, then `ConfigMaps`
.. Click the `Create` button on the top right of the window
.. Select the `cattle-system` namespace
.. Name the ConfigMap
+
[source, console]
----
variables
----
+
.. Copy all of these keys into the first cell in the `Key` column. Each key will automatically get populated into its own cell.
+
[source, console]
----
AWS_ACCESS_KEY_ID
AWS_REGION
DEFAULT_IP_RANGE_START
DEFAULT_IP_RANGE_END
DNSZONE
EMAIL_ADDR
FQDN
JUPYTERHUB_EXTERNAL_IP

----
+
.. Populate the `Value` for each column based on the following guidance:
+
* AWS_ACCESS_KEY_ID is the access key for the {aws-name-full} account that can update records in the {route53-name} DNS zone. E.g. "AK1231231231231231"
*  AWS_REGION is the default region for the {aws-name-full} account. E.g. "us-west-2"
*  DEFAULT_IP_RANGE_START is an OPTIONAL, setting to allow for additional MetalLB IPs. 
*  DEFAULT_IP_RANGE_END is an OPTIONAL, setting to allow for additional MetalLB IPs.
*  DNSZONE is the name of the DNS domain. E.g. "susealliances.com"
*  EMAIL_ADDR is a valid email address to use when creating the Let's Encrypt certificate.
*  FQDN is the fully qualified domain name to reach the {jupyterhub} service. E.g. "jupyterhub.susealliances.com"
*  JUPYTERHUB_EXTERNAL_IP is the IP address that can be resolved publicly to the FQDN. It does not have to be a publicly accessible IP address.
+
.. Click the `Create` button at the bottom right of the window

== Create the variables file 

[NOTE]
====
This part of the procedure will create a {rancher-kubectl-shell-variables-file} in the current session of the {rancher-kubectl-shell}.
This is necessary every time the {rancher-kubectl-shell} is opened.
====

. If the {rancher-kubectl-shell} isn't already open and "Connected", while viewing the {jupyterhub-deployed-name} cluster, select the {rancher-kubectl-shell} icon image::rancher-kubectl-shell.png
. When the {rancher-kubectl-shell} is ready, run the following command:
+
[source, console, subs="attributes+"]
----
kubectl get configmap -n cattle-system variables -o json | jq -r '.data | to_entries[] | "\(.key)=\(.value)"' | sed 's/^/export /' > {rancher-kubectl-shell-variables-file} && cat {rancher-kubectl-shell-variables-file}
----

. If you want to, you can pull the border between the {rancher-kubectl-shell} and the main {rancher} UI down until you need the shell again.

== Deploy {longhorn} on {rke2-full} 

[IMPORTANT]
====
Be sure all nodes in the {rke2} cluster meet the requirements outlined in the <<Prerequisites>> section before preceding with the installation of {longhorn}.
====

Follow the instructions at link:{longhorn-installation-doc}[this URL] to deploy {longhorn} with {rancher-full}.
Be sure to return to this document to edit the *Longhorn Default Settings*.

[TIP]
====
It is beyond the scope of this document to describe managing the {rke2} cluster with third party tools (e.g. kubectl or K9s). 
However, if you have a workstation that meets the requirements described in link:{longhorn-env-check-script-doc}[this document], you can use the script contained therein to verify the {longhorn} nodes meet the requirements before deploying {longhorn}
====

[NOTE]
====
In deploying {longhorn} in the {jupyterhub} {rke2} cluster, we have opted for slightly higher performance and lower storage consumption by configuring {longhorn} to create only two volume replicas per persistent volume. 
This will suit our needs but will sacrifice higher data redundancy. The general guidance here is to use one or two volume replicas, but always one less than the number of nodes in the {rke2} cluster.
====

To configure Longhorn, we have used these specific parameters:

* Version: {longhorn-version}
* Install into Project: System

(Click Next to reach the next screen)

Select *Longhorn Default Settings*

* Check the `Customize Default Settings` checkbox
* Default Replica Count: `2`
* Check the `Replica Node Level Soft Anti-Affinity`

Select *Longhorn Storage Class Settings*

* Default Storage Class Replica Count: `2`

Select *Services and Load Balancing*

* Longhorn UI Service: Rancher-Proxy

(Click Install when ready)

[TIP]
====
After the final message `SUCCESS: helm upgrade --install=true...` is displayed, you can close terminal window and continue with the next steps in this procedure.
====

Finish configuring {longhorn} and verify proper functionality

. From the menu on the left, select `Storage`, then `StorageClasses`
.. If there are any other StorageClasses, aside from the `Longhorn` StorageClass, that show as a "Default", click on the three dots to the right of each one, then select `Reset Default`


== Deploy {metallb} on {rke2-full}

[IMPORTANT]
====
Be sure all nodes in the {rke2} cluster meet the <<Prerequisites>> section before preceding with the installation of {metallb}.
====

Installing {metallb} is accomplished in two, main steps.
First, you will deploy the {metallb} Helm chart from inside {rancher}.
Then, you will provide {metallb} with the virtual IP addresses dedicated for its use.
After installation you will validate that {metallb} is providing virtual IP addresses to applications upon request.

Deploy the {metallb} load balancer based on the following instructions:

. From the {rancher} home screen, select the {k8s} cluster {jupyterhub-deployed-name}
. From the menu on the left select `Apps`, then `Charts`
. In the `Filter` field, type *{metallb}* and select the {metallb} {rancher} chart
+
[NOTE]
====
The version used for this project is {metallb-version}.
====
+
. Click `Install`
. For the `Install into Project` option, select `System`, then click `Next`
. Click `Install`
+
[TIP]
====
After the final message `SUCCESS: helm install...` is displayed, you can close terminal window and continue with the next steps in this procedure.
====
+

Add the dedicated IP address configuration to {metallb}:

. While viewing the {jupyterhub-deployed-name} cluster, select the {rancher-kubectl-shell} icon

. When the {rancher-kubectl-shell} is ready, run the following command:
+
[source, console, subs="attributes+"]
----
kubectl get configmap -n cattle-system variables -o json | jq -r '.data | to_entries[] | "\(.key)=\(.value)"' | sed 's/^/export /' > {rancher-kubectl-shell-variables-file} && cat {rancher-kubectl-shell-variables-file}
----
//. When the {rancher-kubectl-shell} is ready, run this command:
//+
//[IMPORTANT]
//====
//If the follow command fails with the error that the {rancher-kubectl-shell-variables-file} doesn't exist, return to the section <<Create the variables file>> to recreate it.
//====
+
. Create the {metallb} IPAdressPool and L2Advertisement:
+
[source, console, subs="attributes+"]
----
source {rancher-kubectl-shell-variables-file} && \
kubectl apply -f - <<EOF  
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: jupyterhub-pool
  namespace: {metallb-namespace}
spec:
  addresses:
  - ${JUPYTERHUB_EXTERNAL_IP}/32
  autoAssign: false

---

apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: all-pools
  namespace: {metallb-namespace}
EOF
----

[NOTE]
====
(Optional) If additional load balanced IP addresses are desired, complete the following step.
====
+
[TIP]
====
There are many ways to specify the desired IP address range. See the link:{metallb-ipaddress-config-url}[{metallb} documentation] for more information.
====
+

. Paste this command into the {rancher-kubectl-shell}:
+
[IMPORTANT]
====
If the follow command fails with the error that the {rancher-kubectl-shell-variables-file} doesn't exist, return to the section <<Create the variables file>> to recreate it.
====
+
[source, console, subs="attributes+"]
----
source {rancher-kubectl-shell-variables-file} && 

kubectl apply -f - <<EOF  
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: default-pool
  namespace: {metallb-namespace}
spec:
  addresses:
  - ${DEFAULT_IP_RANGE_START}-${DEFAULT_IP_RANGE_END}
EOF
----

Validate that {metallb} provides an IP address to an application and that {longhorn} provides a persistent volume, when they are called upon:

. Paste this command into the {rancher-kubectl-shell}:
//+
//[IMPORTANT]
//====
//If the follow command fails with the error that the {rancher-kubectl-shell-variables-file} doesn't exist, return to the section <<Create the variables file>> to recreate it.
//====
+
[source, console, subs="attributes+"]
----
source {rancher-kubectl-shell-variables-file} && \
kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1
        ports:
        - name: http
          containerPort: 80
        volumeMounts:
        - mountPath: /mnt/test-vol
          name: test-vol
      volumes:
      - name: test-vol
        persistentVolumeClaim:
          claimName: nginx-pvc

---

kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: nginx-pvc
  namespace: default
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

---

apiVersion: v1
kind: Service
metadata:
  name: nginx
  namespace: default
spec:
  ports:
  - name: http
    port: 8080
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx
  type: LoadBalancer
  loadBalancerIP: ${JUPYTERHUB_EXTERNAL_IP}
EOF
----
+
. Verify the pod is `Running`, the longhorn StorageClass is the only `(default)` StorageClass, the persistentvolumeclaim is `Bound`, and the service has an `EXTERNAL-IP`
+
[source, console, subs="attributes+"]
----
kubectl get pod,sc,pvc,svc -n default 
----
+
. Test that the service is reachable through the load balancer IP address:
+
[IMPORTANT]
====
If the follow command fails with the error that the {rancher-kubectl-shell-variables-file} doesn't exist, return to the section <<Create the variables file>> to recreate it.
====
+
[source, console, subs="attributes+"]
----
source {rancher-kubectl-shell-variables-file} && \
kubectl get svc -n default | grep -w nginx > /tmp/externalip
read -r a b c EXTERNAL_IP e f < /tmp/externalip
curl http://${EXTERNAL_IP}:8080 2>&1 | grep "Thank you"
----
+
.. An HTML encoded output should be displayed that includes the phrase "Thank you for using nginx."
+
. When finished with testing, delete the deployment and service 
+
[source, console, subs="attributes+"]
----
kubectl delete deploy nginx -n default
kubectl delete pvc nginx-pvc -n default
kubectl delete service nginx -n default 
----

//If you want to, you can pull the border between the {rancher-kubectl-shell} and the main {rancher} UI down until you need the shell again.

== Deploy {cert-manager} on {rke2-full}

Installing {cert-manager} is accomplished in two main steps:
First, you will install the required {cert-manager} Custom Resources.
Then, you will deploy the {cert-manager} Helm chart from inside {rancher}.
After installation, you will create the TLS certificate and private key for your {jupyterhub} service.

Install the {cert-manager} Custom Resources:
+

. While viewing the {jupyterhub-deployed-name} cluster, select the {rancher-kubectl-shell} icon 

. When the {rancher-kubectl-shell} is ready, execute this command:
+
[source, console, subs="attributes+"]
----
kubectl apply -f {cert-manager-crds-url}
----
+
[TIP]
====
If you want to, you can pull the border between the {rancher-kubectl-shell} and the main {rancher} UI down until you need the shell again.
====

Deploy {cert-manager} based on the following instructions:
+
//. From the {rancher} home screen, select the {k8s} cluster {jupyterhub-deployed-name}

. From the menu on the left select `Apps`, then `Charts`
. In the `Filter` field, type *{cert-manager}* and select the {cert-manager} {rancher} chart
+
[NOTE]
====
The version used for this project is {cert-manager-version}. If the current version isn't displayed at the top, select it from the list on the right.
====
+
. Click `Install`
. For the `Install into Project` option, select `System`, then click `Next`
. Click `Install`
+
[TIP]
====
After the final message `SUCCESS: helm install...` is displayed, you can close terminal window and continue with the next steps in this procedure.
====

== Create the {jupyterhub} TLS certificate and private key through {cert-manager}

. While viewing the {jupyterhub-deployed-name} cluster, select the {rancher-kubectl-shell} icon 
+
. When the {rancher-kubectl-shell} is ready, run the following command:
+
[source, console, subs="attributes+"]
----
kubectl get configmap -n cattle-system variables -o json | jq -r '.data | to_entries[] | "\(.key)=\(.value)"' | sed 's/^/export /' > {rancher-kubectl-shell-variables-file} && cat {rancher-kubectl-shell-variables-file}
----
+
[NOTE]
====
An AWS user with appropriate IAM policies and API access keys is needed for cert-manager to access the Route53 DNS records. 
See the cert-manager documentation at link:{cert-manager-dns01-rout53-doc}[for more information].
====
+
. Run the following command to ensure the fully qualified domain name of the {jupyterhub-deployed-name} service resolves to its dedicated IP address.
+
//[IMPORTANT]
//====
//If the follow command fails with the error that the {rancher-kubectl-shell-variables-file} doesn't exist, return to the section <<Create the variables file>> to recreate it.
//====
+
[source, console, subs="attributes+"]
----
source {rancher-kubectl-shell-variables-file} && \
getent hosts ${FQDN}
----
+
. Create the {jupyterhub-namespace} namespace:
+
[source, console, subs="attributes+"]
----
source {rancher-kubectl-shell-variables-file} && \
kubectl create namespace {jupyterhub-namespace}
----
+
[NOTE]
====
When initially creating the cert-manager Issuer, ensure the `server: {lets-encrypt-base-staging-url}` line is uncommented and the `server: {lets-encrypt-base-prod-url}` line is commented out. 
After verifying that the certificate can be issued correctly, we will reverse this to obtain the valid, production certificate.
====
+

. Create the cert-manager Issuer:
+
[IMPORTANT]
====
If the follow command fails with the error that the {rancher-kubectl-shell-variables-file} doesn't exist, return to the section <<Create the variables file>> to recreate it.
====
+
[source, console, subs="attributes+"]
----
source {rancher-kubectl-shell-variables-file} && \
kubectl apply -f - <<EOF
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: letsencrypt-issuer
  namespace: {jupyterhub-namespace}
spec:
  acme:
    email: ${EMAIL_ADDR}
    server: {lets-encrypt-full-staging-url} # Use this line to test the process of issuing a certificate to avoid the Let's Encrypt production rate limits
#    server: {lets-encrypt-full-prod-url} # Use this line after the certificate issues correctly
    privateKeySecretRef:
      name: letsencrypt-issuer-priv-key # K8s secret that will contain the private key for this, specific issuer
    solvers:
    - selector:
        dnsZones: 
          - "${DNSZONE}"
      dns01:
        route53:
          region: ${AWS_REGION}
          accessKeyID: ${AWS_ACCESS_KEY_ID}
          secretAccessKeySecretRef:
            name: route53-credentials-secret
            key: secret-access-key
EOF
----
+
. Set the variable that contains your AWS secret access key that matches the AWS access key for the account you are using for this project:
+
[source, console, subs="attributes+"]
----
  export AWS_SECRET_ACCESS_KEY=""
----
+
. Create the Kubernetes secret containing the aws_secret_access_key for the AWS user:
+
[source, console, subs="attributes+"]
----
source {rancher-kubectl-shell-variables-file} && \
kubectl create -n {jupyterhub-namespace} secret generic route53-credentials-secret --from-literal=secret-access-key=${AWS_SECRET_ACCESS_KEY}
----
+
. Verify the contents of the secret:
+
[source, console, subs="attributes+"]
----
kubectl get -n {jupyterhub-namespace} secret route53-credentials-secret -o jsonpath={.data.secret-access-key} | base64 -d; echo ""
----
+
. Create the cert-manager Certificate resource file:
+
[IMPORTANT]
====
If the follow command fails with the error that the {rancher-kubectl-shell-variables-file} doesn't exist, return to the section <<Create the variables file>> to recreate it.
====
+
[source, console, subs="attributes+"]
----
source {rancher-kubectl-shell-variables-file} && \
kubectl apply -f - <<EOF
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: {jupyterhub-deployed-name}-staging-certificate
  namespace: {jupyterhub-namespace}
spec: 
  secretName: {jupyterhub-deployed-name}-staging-certificate-secret # Kubernetes secret that will contain the tls.key and tls.crt of the new cert
  commonName: ${FQDN}
  dnsNames:
    - ${FQDN}
  issuerRef:
    name: letsencrypt-issuer
    kind: Issuer
EOF
----
+
. Check the status of the certificate:
+
[source, console, subs="attributes+"]
----
kubectl get -w -n {jupyterhub-namespace} certificate
----
+
.. Use `Ctrl+c` to exit the kubectl -w (watch) command
+
[NOTE]
====
The certificate commonly takes 100 seconds to be issued but can take up to three minutes. 
The `READY` status will change to `True` when it is issued.
====
+
. If needed, check the progress of the certificate:
+
[source, console, subs="attributes+"]
----
kubectl describe -n {jupyterhub-namespace} certificate {jupyterhub-deployed-name}-staging-certificate
----
+
[IMPORTANT]
====
If the certificate seems to be taking a long time to be issued, review the cert-manager logs for clues. 
Common errors are related to DNS resolution, credentials, and IAM policies. 
Keep checking back for the status of the certificate since it will likely keep working in the background.
====
+
. If needed, review the cert-manager logs:
+
[source, console, subs="attributes+"]
----
kubectl logs -n cert-manager -l app=cert-manager
----
+
[IMPORTANT]
====
Proceed to the next section only after the certificate shows a `READY` status of `True` 
====
+
. Edit the `letsencrypt-issuer` to change from the {lets-encrypt} staging server to the production server:
+
[source, console, subs="attributes+"]
----
kubectl -n {jupyterhub-namespace} edit issuer letsencrypt-issuer
----
+
[NOTE]
====
The {lets-encrypt} URLs are as follows:

Staging: {lets-encrypt-full-staging-url}

Production: {lets-encrypt-full-prod-url}
====
+
. Remove the staging certificate and its associated secret:
+
[source, console, subs="attributes+"]
----
kubectl -n {jupyterhub-namespace} delete secret {jupyterhub-deployed-name}-staging-certificate-secret
kubectl -n {jupyterhub-namespace} delete certificate {jupyterhub-deployed-name}-staging-certificate
----
+
. Create the production certificate:
+
[IMPORTANT]
====
If the follow command fails with the error that the {rancher-kubectl-shell-variables-file} doesn't exist, return to the section <<Create the variables file>> to recreate it.
====
+
[source, console, subs="attributes+"]
----
source {rancher-kubectl-shell-variables-file} && \
kubectl apply -f - <<EOF
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: {jupyterhub-deployed-name}-certificate
  namespace: {jupyterhub-namespace}
spec: 
  secretName: {jupyterhub-deployed-name}-certificate-secret # Kubernetes secret that will contain the tls.key and tls.crt of the new cert
  commonName: ${FQDN}
  dnsNames:
    - ${FQDN}
  issuerRef:
    name: letsencrypt-issuer
    kind: Issuer
EOF
----
+
. Check the status of the certificate and secret:
+
[source, console, subs="attributes+"]
----
kubectl get -n jupyterhub certificates,secrets
----





== Deploy {jupyterhub} on {rke2-full}

[NOTE]
====
It is beyond the scope of this document to configure {jupyterhub} to use an authentication method. 
By default, {jupyterhub} will allow any user to log in with or without a password.
See the {jupyterhub-auth-doc}[{juyterhub} authentication documentation] for information on integrating {jupyterhub} with a central authencation application.
====

[IMPORTANT]
====
Be sure you have met the requirements outlined in the <<Prerequisites>> section before preceding with the installation of {jupyterhub}.
Specifically, you must possess a DNS domain registered with a public DNS provider. In this guide we will use the Fully Qualified Domain Name of {public-fqdn}
====

Follow the instructions at the link:{jupyterhub-installation-doc}[{jupyterhub} installation documentation] to install {jupyterhub} on {rke2} using its Helm chart

[NOTE]
====
The instructions below will guide you through the process of creating a config.yaml file that will be required by the Helm installation command.
====

Create the config.yaml file that will point to the secret containing TLS certificate and private key. 

. While viewing the {jupyterhub-deployed-name} cluster, select the {rancher-kubectl-shell} icon 
+
. When the {rancher-kubectl-shell} is ready, run the following command:
+
[source, console, subs="attributes+"]
----
kubectl get configmap -n cattle-system variables -o json | jq -r '.data | to_entries[] | "\(.key)=\(.value)"' | sed 's/^/export /' > {rancher-kubectl-shell-variables-file} && cat {rancher-kubectl-shell-variables-file}
----
+
. Create the config.yaml file:
+
[source, console, subs="attributes+"]
----
source {rancher-kubectl-shell-variables-file} && \
cat <<EOF> config.yaml
proxy:
  https:
    enabled: true
    hosts:
      - jupyter.susealliances.com
    type: secret
    secret:
      name: {jupyterhub-deployed-name}-certificate-secret
  service:
    loadBalancerIP: ${JUPYTERHUB_EXTERNAL_IP}
EOF
----
+
. Add the {jupyterhub} Helm chart repo:
+
[source, console, subs="attributes+"]
----
helm repo add jupyterhub https://hub.jupyter.org/helm-chart/
helm repo update
----
+
. Find the current `CHART VERSION` for `jupyterhub/jupyterhub`:
+
[source, console, subs="attributes+"]
----
helm search repo jupyter
----
+
. Set this variable with the `CHART VERSION`:
+
[source, console, subs="attributes+"]
----
export CHART_VERSION=""
----
+
. Install {jupyterhub}:
+
[source, console, subs="attributes+"]
----
helm upgrade --cleanup-on-fail \
  --install jupyterhub jupyterhub/jupyterhub \
  --namespace jupyterhub \
  --create-namespace \
  --version=${CHART_VERSION} \
  --values config.yaml
----



// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Detail the steps of the installation procedure.
// - The reader should be able to copy and paste commands to
//   a local environment or follow along locally with screenshots.
// - Be sure tp include verifications that the reader can use to
//   validate that major steps were performed correctly.
//
// - Make use of:
//   - Ordered lists
//   - Code blocks
//   - Screenshots
//   - Admonitions
//
// - If multiple installation methods are to be detailed, then
//   - Create a summary list here.
//   - Detail each method in a subsection.
//
//   NOTE: For solutions involving SUSE Rancher, it is preferred
//         to highlight installation through the Rancher Apps Catalog
//         with appropriate screenshots and SUSE branding.
//         Alternatively or additionally, installation via the
//         command line can be documented.
//
// - Complex installations may be broken into one or more subsections.
//
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =



== Demonstration

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Illustrate functionality of the solution for a use case.
//
// NOTE: This demonstration text could be used to script a
//       demonstration video.
//
// - Typical demonstration flow is:
//   1. Outline the demonstration.
//   2. Prepare the environment.
//      This should be minimal, such as downloading some data to use.
//   3. Perform the demonstration.
//   4. Do not overuse screenshots.
//   5. Clean up the environment.
//
// - Make use of ordered lists, code blocks, admonitions, and screenshots.
//
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =


To validate the operation of your new {jupyterhub} installation, we will clone the {jupyterlab-provider} {github-name} repository, then load and run one of its notebooks

. Connect to your deployed {jupyterhub} fully qualified domain name. In our example it is {public-fqdn}.

. Log in with your user name
+
[NOTE]
====
If an identity provider hasn't been specifically integrated with {jupyterhub}, it will accept any user name as a log in and does not require a password.
====
+
. From the {jupyterlab} Launcher, select the `Terminal` session
. When the session is ready, clone the {jupyterlab-provider} repository:
+
[source, console, subs="attributes+"]
----
git clone https://github.com/jupyter/notebook.git
----
+
. After the repository has been downloaded, use the file browser tool at the top left of the window to navigate to `notebook` -> `docs` -> `source` -> `examples` -> `Notebook`
. Open the file `{jupyter-notebooks-running-code-filename}` by double-clicking on it
. From the top menu, select `Edit` -> `Clear Outputs of All Cells`
. You can press "Shift+Enter" to run the selected cell and advance to the next cell, or click the equivilent button at the top of the tab
. Continue to work with this notebook as long as you'd like to or until you feel your new {jupyterhub} installation is working correctly.


== Summary

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Summarize:
// - Motivation for the guide (1 sentence)
// - What was covered (1-2 sentences)
// - Next steps (unordered list of 2-4 further learning resources)
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

In this document you have gone through the process of configuring {harvester} to create the VMs needed for your {jupyterhub} deployment, deployed {rke2-full} onto those VMs, then added {longhorn}, {metal-lb}, and {cert-manager}.
Finally, you have installed {jupypterhub} and verified that the installation is working correctly. 
Several customizations are outside of the scope of this document. 
For information on authenication provider integration see {jupyterhub-auth-doc}[this document].
For information on {k8s} specfic customizations see {jupyterhub-kubernetes-customizations-doc}[this document]. 
For general configuration information see {jupyterhub-docs}[this document].



// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Do not modify below this break.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

++++
<?pdfpagebreak?>
++++


:leveloffset: 0

== Legal notice
include::common_trd_legal_notice.adoc[]

++++
<?pdfpagebreak?>
++++


:leveloffset: 0
include::common_gfdl1.2_i.adoc[]

//end
