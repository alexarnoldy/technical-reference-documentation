:docinfo:
include::./common_docinfo_vars.adoc[]

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// GENERAL COMMENTS
//   - See the SUSE TRD Contributors Guide for detailed guidance:
//     https://documentation.suse.com/trd/contributors/single-html/suse-trd_contrib-guide/
//   - Keep in mind that this is a "getting started" guide.
//   - Write to the audience you are trying to reach.
//   - Add or remove sections and subsections as needed.
// 
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =


// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// DOCUMENT ATTRIBUTES AND VARIABLES
//
// NOTES:
// 1. Define and use document attributes or variables in this file.
// 2. Update the docinfo.xml file if needed.
//
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =


// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// ORGANIZATION
//   Do NOT modify this section.
// -
:trd: Technical Reference Documentation
:type: Getting Started
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -


// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// DOCUMENT REVISION DATE
//-
:revision-date: YYYY-MM-DD
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -


// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// DOCUMENT TITLE AND SUBTITLE
//   Provide concise but descriptive title and subtitle.
//   title - (<75 characters) concisely identify the guide.
//           (e.g.: "Kubeflow with Rancher")
//   subititle - (<75 characters) expound on the title.
//               (e.g., "Deploying Kubeflow onto an RKE2 Cluster with Rancher")
// -
:title: (<75 characters) Your Guide Title
:subtitle: (<75 characters) Your Guide Subtitle
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -


// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// TECHNICAL COMPONENTS
//   Identify the technical components featured in the guide.
//   Use variables to store information about the solution components.
//   This makes it much easier to update the document for future
//   version upgrades, URL changes, etc.
//
//   Variable names follow a simple pattern:
//   - for SUSE components: scompX[-MODIFIER]
//   - for partner/project components: pcompX[-MODIFIER]
//   where:
//   - X is an integer starting from 1 for the primary component,
//     2 for a secondary component, and so on.
//   - -MODIFIER is text that identifies the stored information.
//   Some -MODIFIER values include:
//   - EMPTY: If the modifier is missing, the variable contains
//     a short name for the component (e.g., 'SLES', 'Rancher Prime')
//   - -full: Long name of the component
//     (e.g., 'SUSE Linux Enterprise Server', 'Rancher Prime by SUSE')
//   - -provider: Name of company or project providing the component
//     (e.g., 'SUSE', 'HPE', 'Kubeflow', 'Veeam')
//   - -version: Relevant product version (e.g., '15', '15SP5', '2.7')
//     or versions (e.g., '15SP4, 15SP5', '15SP3+', '2.6+', '2.x')
//   - -website: Product website (e.g., https://www.suse.com/products/server/)
//   - -docs: Product documentation (e.g., https://documentation.suse.com/sles/)
//
//   You can create additional modifiers as needed.
//  
// -
// SUSE Components
// -

:rke2: RKE2
:rke2-full: Rancher Kubernetes Engine 2
:rke2-url: https://www.suse.com/products/rancher-kubernetes-engine/
:rke2-support-matrix: https://www.suse.com/suse-rke2/support-matrix/all-supported-versions/
:rke2-docs: https://docs.rke2.io/
:rke2-version: v1.27.8+rke2r1

:rancher: Rancher
:rancher-full: Rancher by SUSE
:rancher-url: https://www.suse.com/solutions/enterprise-container-management/#rancher-product
:rancher-docs: https://ranchermanager.docs.rancher.com/
:rancher-docs-rke2: {rancher-docs}pages-for-subheaders/launch-kubernetes-with-rancher#rke2
:rancher-kubectl-url: https://www.suse.com/c/rancher_blog/how-to-manage-kubernetes-with-kubectl/
:rancher-kubectl-shell: Kubectl Shell

:longhorn: Longhorn
:longhorn-url: https://longhorn.io/
:longhorn-docs: https://longhorn.io/docs/latest
:longhorn-installation-doc: https://ranchermanager.docs.rancher.com/integrations-in-rancher/longhorn/overview#installing-longhorn-with-rancher
:longhorn-env-check-script-doc: https://longhorn.io/docs/latest/deploy/install/#using-the-environment-check-script


:harvester: Harvester
:harvester-url: https://www.rancher.com/index.php/products/harvester
:harvester-version: v1.3
:harvester-docs: https://docs.harvesterhci.io/

:harvester-upload-image-url: https://docs.harvesterhci.io/v1.3/upload-image
:harvester-create-rke2-cluster-url: https://docs.harvesterhci.io/v1.3/rancher/node/rke2-cluster/#create-rke2-kubernetes-cluster
:harevester-cloud-configuration-url: https://docs.harvesterhci.io/v1.3/vm/index#cloud-configuration
:harvester-namespace: default
:harvester-cloud-config-template-name: rke2-ready

:sles: SLES
:sles-full: SUSE Linux Enterprise Server
:sles-minimal: SLES Minimal Cloud
:sles-version: 15
:sles-sp: 5
:sles-sp-full: SP{sles-sp}
:sles-url: https://www.suse.com/products/server/
:sles-docs: https://documentation.suse.com/sles/{sles-version}-{sles-sp-full}/
:sles-docs-modules: {sles-docs}single-html/SLES-modules/#sec-modules-install
:sles-minimal-download-url: https://www.suse.com/download/sles/
:sles-minimal-file-name: SLES15-SP5-Minimal-VM.x86_64-Cloud-GM.qcow2
:sles-minimal-saved-name: sles-{sles-version}-{sles-sp}-minimal
// -
// Partner/Project Components
// -
:jupyterhub-provider: Project Jupyter
:jupyterhub: JupyterHub
:jupyterhub-version: 4.0.2
:jupyterhub-website: https://jupyter.org/hub
:jupyterhub-docs: https://jupyterhub.readthedocs.io/en/latest/
:jupyterhub-installation-doc: https://z2jh.jupyter.org/en/stable/jupyterhub/installation.html

:jupyterhub-deployed-name: jupyterhub

:jupyterlab-provider: Project Jupyter
:jupyterlab: JupyterLab
:jupyterlab-version: 4.0.9
:jupyterlab-website: https://jupyter.org/
:jupyterlab-docs: https://jupyterlab.readthedocs.io/en/latest/

:metallb: MetalLB
:metallb-url: https://metallb.org/
:metallb-docs: https://metallb.org/concepts/
:metallb-version: 0.13.12
:metallb-ipaddress-config-url: https://metallb.org/configuration/#defining-the-ips-to-assign-to-the-load-balancer-services

:k8s: Kubernetes
:k8s-version: 1.27
:harvester-cloud-creds-url: https://docs.harvesterhci.io/v1.3/rancher/node/rke2-cluster#create-your-cloud-credentials

:lets-encrypt: Let's Encrypt
:public-domain-name: susealliances.com
:public-fqdn: {jupyterhub-deployed-name}.{public-domain-name}

:certbot-name: certbot
:certbot-package-name: python3-{certbot-name}
:certbot-dns-plugin-url: https://eff-certbot.readthedocs.io/en/stable/using.html#dns-plugins

:aws-name-full: Amazon Web Services
:route53-name: Amazon Route 53
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -


// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// DOCUMENT DESCRIPTIONS
//   Describe the guide by its purpose, use case, value.
//
// usecase: (<55 characters) key words or phrases that identify the
//          use case of this guide
//          (e.g.: "Database-as-a-Service", "edge analytics in healthcare",
//                 "Kubernetes-native object storage")
// description: (<150 characters) brief description of what this guide
//              provides (e.g.: "Deploy Kubeflow with Rancher Primer")
// description-short: (<55 characters) condensed description suitable for
//                    social media (e.g.: "Kubeflow with Rancher")
// executive-summary: (<300 characters) brief summary of the guide that
//                    appears near the beginning of the rendered document
//                    (e.g.: "Kubeflow simplifies deployment of machine
//                            learning (ML) workflows on Kubernetes clusters.
//                            This document provides step-by-step guidance
//                            for deploying Kubeflow on an RKE2 cluster with
//                            Rancher."
//
// -

:title: Deploy {jupyterhub} with {rancher-full}

:subtitle: Deploying {jupyterhub} with {rancher} using {longhorn} for highly available storage, {metallb} for load balancing, and {harvester} for compute virtualization.

:usecase: provide access to {jupyterlab} to teams of Developers and Data Scientists

:description: Deploy {jupyterhub} on {rke2} with {rancher-full}, {longhorn}, and {harvester}.

:description-short: Deploy {jupyterhub} with {rancher-full}

:executive-summary: Enable Developers and Data Scientists to take full advantage of {jupyterlab} from any device that provides a modern web browser.

// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -


// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// CONTRIBUTORS
//   Specify information about authors, editors, and other
//   contributors here.
//   Follow the pattern to provide fist name, surname, job title,
//   and organization name for each contributor.
//   NOTE: To list additional authors or others on the cover page,
//         you must edit the docinfo.xml file as well.
// -
:author1_firstname: Alex
:author1_surname: Arnoldy
:author1_jobtitle: Global Alliance Solutions Architect
:author1_orgname: SUSE

:editor1_firstname: Terry
:editor1_surname: Smith
:editor1_jobtitle: Director of Global Partner Solutions
:editor1_orgname: SUSE

//:author2-firstname: first (given) name
//:contrib1-firstname: first (given) name
//:editor1-firstname: first (given) name
// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -


// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
// MISCELLANEOUS
//   Define any additional variables here for use within the document.
// -


// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -


= {title}: {subtitle}



== Introduction

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Provide a motivation for the document in 2-4 sentences to identify:
//   - what the document is about
//   - why it may be of interest to the reader
//   - what products are being highlighted
// Include an approved SUSE | Partner logo lock-up if possible
// Include any additional, relevant details
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =


{jupyterlab} provides a flexible, interactive environment where you develop code, explore data, and build visualizations, all within a single interface. 
{jupyterlab} is perfect for individual research, learning, and prototyping. 
However, {jupyterlab} runs as a local application, requiring each user's time and knowledge to install and maintain it.

{jupyterhub} provides all of the value of {jupyterlab}, from a centrally maintained {k8s} cluster. 
This removes the burden of installation and maintenance for the {jupyterlab} users, allowing them to get the full value of the tool from any device that provides a web browser. 
In addition {jupyterhub} fuels collaboration with the ability to work in individual or shared development environments, a powerful capability that isn't available to individual {jupyterlab} installations.  
This guide shows you how to unlock the full potential of {jupyterhub} by deploying through Rancher by SUSE, enabling scalable deployments, centralized management, and enhanced security. 



=== Scope

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Specify what this guide covers in no more than 2 sentences.
//   E.g., "You will learn how to ..."
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =


In this guide we will cover:

* creating a highly-available {rke2} cluster using {rancher} and {harvester}

* installing {longhorn} with {rancher}

* installing and configuring {metallb} with {rancher}

* installing {jupyterhub} with static authentication for a handful of users

* creating a sample workload in {jupyterhub} that demonstrates both private and collaborative workspaces

=== Audience

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Identify for whom this document is intended, perhaps with:
//   - topics of interests (e.g., machine learning, security, etc.)
//   - job roles (e.g., developer, administrator, platform architect, etc.)
//   - required skills
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

This guide is intended for high technology professionals (including {k8s} administrators, proficient MLOps practitioners, and application developers) seeking to unlock the full potential of their collaborative data science and development environment.

To be successful, you need a foundational understanding of {k8s}, {rancher}, and {jupyterlab}.


=== Acknowledgements

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Acknowledge contributors not listed as authors.
// Something like:
//   The authors wish to acknowledge the contributions of the following
//   individuals:
//   * {contrib1-firstname} {contrib1-surname}, {contrib1-jobtitle}, {contrib1-orgname}
//   * {contrib2-firstname} {contrib2-surname}, {contrib2-jobtitle}, {contrib2-orgname}
// NOTE: If there are none, comment out this section.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

The author wants to acknowledge contributions to this guide by:

* {contrib1_firstname} {contrib1_surname}, {contrib1_orgname}, {contrib1_jobtitle}
//* {contrib2_firstname} {contrib2_surname}, {contrib2_orgname}, {contrib2_jobtitle}
* {editor1_firstname} {editor1_surname}, {editor1_orgname}, {editor1_jobtitle}



== Prerequisites

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Identify minimum requirements (prerequisites) the reader
// would need in order to follow the steps of this guide.
// - Use an unordered list.
// - Link to existing resources whenever possible.
// - Keep this section brief but elaborate as needed.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

Before you embark on the procedures outlined in this guide, ensure that your environment is ready with the resources listed here.

.*{rancher} installation*
//
+
{rancher} can be running in a public cloud provider or on-premises, but needs needs to have access to the infrastructure provider for the {rke2} cluster.

. A {harvester} cluster 
//
+
{harvester} will be the infrastructure provider for the {rke2-full} cluster. {rancher} must be configured with cloud credentials with full access the {harvester}. See {harvester-cloud-creds-url} for more information.
//
+
[NOTE]
====
We have selected the {harvester} infrastructure provider because of its seamless integration with {rancher}, and its ability to provide a low-cost, cloud-like Infrastructure as a Service experience for its users. 
If you don't have a {harvester} cluster readily available, {rke2} can be deployed on another infrastructure provider, or a hosted {k8s} can be used.
====
+

.*Three or more available IP addresses*
//
+
The {metallb} load balancer needs at least three IP addresses dedicated to its operation. This requirement can be differed if there is an external load balancer available (e.g. a public cloud application load balancer).
+

.*TLS certificate and private key*
//
+
A TLS certificate and private key from a publicly recognized provider are required to provide full HTTPS security when connecting to {jupyterhub}. 
This guide will include guidance for obtaining these pieces from {lets-encrypt}. 
{lets-encrypt} will require that you possess the credentials to make changes to a publicly resolvable DNS domain, therefore you must also possess a publicly registered DNS domain name with API credentials to add and remove records from it.
+
[NOTE]
====
While it is not recommended, this deployment can be completed with HTTP communication, sacrificing security for simplicity. 
This should never be done in production, or otherwise valuable, environments.
====
+
[IMPORTANT]
====
It is out of the scope of this document to cover the process for providing TLS certificates from a private CA.
====
+
One {sles} {sles-version} {sles-sp-full} system needs to have certbot installed on it to obtain the {lets-encrypt} TLS certificate.
Install certbot with these commands: 
. sudo SUSEConnect -p PackageHub/{sles-version}.{sles-sp}/x86_64
. sudo zypper install {certbot-package-name}
+
Review the {certbot-name} information for link:{certbot-dns-plugin-url}[connecting to your DNS provider]
+
[NOTE]
====
For this procedure we are using {route53-name} from {aws-name-full}.
We will provide {certbot-name} with {route53-name} credentials through environmental variables.
====
+
. Switch to the root user account:
+
[source, console, subs="attributes+"]
----
sudo -i
----
+
. Set the credential variables that can modify the DNS zone in {route53-name}, as well as the fully qualified domain name for {jupyterhub}
+
[source, console, subs="attributes+"]
----
export AWS_ACCESS_KEY_ID=""
export AWS_SECRET_ACCESS_KEY=""
export FQDN="{public-fqdn}"
----
+
. Run {certbot-name} to generate the {lets-encrypt} TLS certifiate and associated private key
+
[source, console, subs="attributes+"]
----
/usr/bin/certbot certonly --cert-name ${FQDN} --dns-route53 -d ${FQDN}
----
+
[NOTE]
====
The TLS certificate will be at {crt-path} /etc/letsencrypt/live/{public-fqdn}/fullchain.pem
The private key will be at {key-path} /etc/letsencrypt/live/{public-fqdn}/privkey.pem
====

//. A publicly resolvable domain name
//
//You can easily obtain a domain name from most public DNS providers, for a small fee. 
//You will need this for Let's Encrypt to verify your ownership of the {jupyterhub} service and provide the required TLS certificates for HTTPS communication. 
//See this link:https://community.letsencrypt.org/t/dns-providers-who-easily-integrate-with-lets-encrypt-dns-validation/86438[blog post] for a general list of DNS providers.


.*Required software is installed on each node of the {rke2} cluster*
//
+
[NOTE]
====
This requirement can only be completed after the {rke2} cluster has been deployed. 
After deploying the {rke2} cluster, you can return here to complete this step.
====
+
Longhorn leverages several open source software projects, including open-iscsi and nfs-client, for core functionality. 
You must ensure that each node in the {rke2} cluster meets link:https://longhorn.io/docs/1.5.3/deploy/install/#installation-requirements[these requirements].
+
Most of the required software can be installed on {sles-full} with this command:
+
[source, console]
----
sudo zypper in coreutils jq nfs-client util-linux open-iscsi
----



== Overview

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Provide an overview of the solution and the processes detailed
// in this guide.
// - Identify components
// - Describe how the components fit together
// - Leverage diagrams as appropriate, including (but not limited to):
//   - component architecture
//   - data flow diagram
//   - workflow diagram
// - Summarize the major steps the reader will perform
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

This Getting Started guide will help you build a production quality JupyterHub environment running on an {rke2} {k8s} cluster deployed on virtual machines provided by the {harvester} hypervisor. 
We'll navigate the journey step-by-step, covering:
//
+
* Deploying {rke2-full} on {harvester}: You'll leverage {rancher-full} for {k8s} application and cluster management. 
+
* Configuring {longhorn} for for highly available, persistent storage.
+
* Configuring {metallb}: This versatile load balancer provides virtual IP addresses that ensures traffic to your {jupyterlab} server instances isn't interrupted, even after a node failure.
+
* Installing {jupyterhub}: Using its Helm chart, we'll seamlessly integrate {jupyterhub} within your RKE2 cluster, equipping you with a dynamic platform for interactive data analysis and collaboration.

== Configure {harvester} to deploy properly configured virtual machines

Download {sles-minimal-file-name} image from link:{sles-minimal-download-url}[this URL] (You may need to log in or create a free account).
+
Following link:{harvester-upload-image-url}[these instructions], upload the {sles-minimal} image to {harvester}
+
[NOTE] 
====
In uploading the {sles-minimal} image to {harvester}, we used these parameters:
* Namespace: `{harvester-namespace}`
* Name: `{sles-minimal-saved-name}`
====

Create a `Cloud Config Template` based on link:{harevester-cloud-configuration-url}[these instructions].
+
[NOTE]
====
For this procedure, we have named this Cloud Configuration Template {harvester-cloud-config-template-name}
====
+
[NOTE]
====
If you prefer, configure a more secure password for the root and sles users.
====
+
[IMPORTANT]
====
Update only one the `SUSEConnect` commands below to register the VM operating systems either via an RMT server URL, or with a subscription code (i.e. regcode). 
Be sure to remove the other command.
====
+
[source, console, subs="attributes+"]
----
### cloud-init
#cloud-config
chpasswd:
  list: |
    root:SUSE
    sles:SUSE
  expire: false
runcmd:
#  - SUSEConnect --url http://rmt.example.com
#  - SUSEConnect --regcode MyActivationCode --email MyEmailAddress
#  - SUSEConnect -p sle-module-containers/{sles-version}.{sles-sp}/x86_64   ## Can likely be removed. Need to test.
  - zypper -n in -t pattern apparmor
  - zypper -n in iptables coreutils jq nfs-client util-linux open-iscsi
  - zypper -n up
  - zypper in --force-resolution --no-confirm --force kernel-default
  - systemctl enable iscsid
  - systemctl start iscsid
----

== Deploying {rke2-full} on {harvester}

Follow the instructions at link:{harvester-create-rke2-cluster-url}[this URL] to create an {rke2} cluster using {rancher} and {harvester}.

[NOTE]
====
In creating the {rke2} cluster, we have used these specific parameters:

*Machine Pools*

* Cluster Name: `jupyterhub`
* Machine Count: `3`
* Roles: `etcd`, `Control Plane`, and `Worker`
* CPUs (per node): `8` (4 can be used for smaller deployments)
* Memory (per node): `16` (8 can be used for smaller deployments)
* Namespace: `{harvester-namespace}`
* SSH User: `sles`
* Image: {sles-minimal-saved-name}
* Network: (Select a network to which all {jupyterlab} clients can connect)

*Under "Show Advanced"* 

*User Data* 

* Uncheck the `Install guest agent` checkbox.
* User Data Template: {harvester-cloud-config-template-name}
** Ensure the cloud configuration template data shown is the same as you configured earlier
* Check the `Install guest agent` checkbox.

*Cluster Configuration*

* Kubernetes Version: {rke2-version}
====

[TIP]
====
When selecting the `User Data` template, some browsers won't pull in the correct data if the `Install guest agent` checkbox is selected. To ensure a consistent experience, you first unchecked that box, selected the correct user data, then checked that box again.
====

After the {rke2} cluster has been deployed, its status will become "Active". 
This can take five to ten minutes, depending on the underlying infrastructure. 

Please continue this procedure only after its status shows "Active".


== Deploying {longhorn} on {rke2-full} 

[IMPORTANT]
====
Be sure all nodes in the {rke2} cluster meet the requirements outlined in the <<Prerequisites>> section before preceding with the installation of {longhorn}.
====
+
Follow the instructions at link:{longhorn-installation-doc}[this URL] to deploy {longhorn{ with {rancher-full}
+
[TIP]
====
It is beyond the scope of this document to describe managing the {rke2} cluster with third party tools (e.g. kubectl or K9s). 
However, if you have a workstation that meets the requirements described in link:{longhorn-env-check-script-doc}[this document], you can use the script contained therein to verify the {longhorn} nodes meet the requirements before deploying {longhorn}
====
+
[NOTE]
====
In deploying {longhorn} in the {jupyterhub} {rke2} cluster, we have opted for slightly higher performance and lower storage consumption by configuring {longhorn} to create only two volume replicas per persistent volume. 
This will suit our needs but will sacrifice higher data redundancy.

To configure this, we have used these specific parameters:

* Install into Project: System

*Longhorn Default Settings*

* Check the `Customize Default Settings` checkbox
* Default Replica Count: `2`

*Longhorn Storage Class Settings*

* Default Storage Class Replica Count: `2`

*Services and Load Balancing*

* (Optional) Longhorn UI Service: Rancher-Proxy
====

[TIP]
====
After the final message `SUCCESS: helm upgrade --install=true...` is displayed, you can close terminal window and continue with this procedure.
====

.Finish configuring {longhorn} and verify proper functionality

. From the menu on the left, select `Storage`, then `StorageClasses`
.. If there are any other StorageClasses that show as a "Default", click on the three dots to the right of each one, then select `Reset Default`
. Also under the `Storage` menu item, select `PersistentVolumeClaims`
.. Click `Create`
.. Provide any name such as `test`
.. Ensure the `Storage Class` dropdown says `Use the default class` 
.. Click `Create`
.. After a few seconds the new PVC should change from `Pending` to `Bound`
.. Check the checkbox next to the new PVC, then click `Delete`

== Deploying {metallb} on {rke2-full}

[IMPORTANT]
====
Be sure all nodes in the {rke2} cluster meet the <<Prerequisites>> section before preceding with the installation of {metallb}.
====

Installing {metallb} is accomplished in two, main steps.
First, you will deploy the {metallb} Helm chart from inside {rancher}.
Then, you will provide {metallb} with the virtual IP addresses dedicated for its use.
Finally, you will validate that {metallb} is providing virtual IP addresses to applications upon request.

Deploy the {metallb} load balancer based on the following instructions:
+
. From the {rancher} home screen, select the {k8s} cluster {jupyterhub-deployed-name}
. From the menu on the left select `Apps`, then `Charts`
. In the `Filter` field, type {metallb} and select the {metallb} {rancher} chart
+
[NOTE]
====
The version used for this project is {metallb-version}.
====
+
. Click `Install`
. For the `Install into Project` option, select `System`, then click `Next`
. Click `Install`
+
[TIP]
====
After the final message `SUCCESS: helm install...` is displayed, you can close terminal window and continue with this procedure.
====
+
. Add the dedicated IP address configuration to {metallb}:
.. While viewing the {jupyterhub-deployed-name} cluster, select the {rancher-kubectl-shell} icon image::
.. When the {rancher-kubectl-shell} is ready, create these shell variables:
+
[source, console, subs="attributes+"]
----
export IP_RANGE_START=
export IP_RANGE_END=
----
+
[TIP]
====
There are many ways to specify the desired IP address range. See the link:{metallb-ipaddress-config-url}[{metallb} documentation] for more information.
====
+
.. Paste this command into the {rancher-kubectl-shell}:
+
[source, console, subs="attributes+"]
----
kubectl apply -f - <<EOF  
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: default-pool
  namespace: metallb-system
spec:
  addresses:
  - ${IP_RANGE_START}-${IP_RANGE_END}

---

apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: all-pools
  namespace: metallb-system
EOF
----
+
. Validate that {metallb} provides an IP address to an application when called upon:
.. Paste this command into the {rancher-kubectl-shell}:
+
[source, console, subs="attributes+"]
----
kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1
        ports:
        - name: http
          containerPort: 80
        volumeMounts:
        - mountPath: /mnt/test-vol
          name: test-vol
      volumes:
      - name: test-vol
        persistentVolumeClaim:
          claimName: nginx-pvc

---

kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: nginx-pvc
  namespace: default
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

---

apiVersion: v1
kind: Service
metadata:
  name: nginx
  namespace: default
spec:
  ports:
  - name: http
    port: 8080
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx
  type: LoadBalancer
EOF
----
+
.. Verify the pod is `Running`, the longhorn StorageClass is the `(default)``, the persistentvolumeclaim is `Bound`, and the service has an `EXTERNAL-IP`
+
[source, console, subs="attributes+"]
----
kubectl get pod,sc,pvc,svc -n default 
----
+
.. Test that the service is reachable through the load balancer IP address
+
[source, console, subs="attributes+"]
----
kubectl get svc -n default | grep -w nginx > /tmp/externalip
read -r a b c EXTERNAL_IP e f < /tmp/externalip
curl http://${EXTERNAL_IP}:8080 2>&1 | grep "Thank you"
----
+
... An HTML encoded output should be displayed that includes the phrase "Thank you for using nginx."
+
. When finished with testing, delete the deployment and service 
+
[source, console, subs="attributes+"]
----
kubectl delete deploy nginx -n default
kubectl delete pvc nginx-pvc -n default
kubectl delete service nginx -n default 
----

Close the {rancher-kubectl-shell} and move on to the next step of this procedure.

== Deploying {juypterhub} on {rke2-full}

[IMPORTANT]
====
Be sure you have met the requirements outlined in the <<Prerequisites>> section before preceding with the installation of {jupyterhub}.
Specifically, you must possess a DNS domain registered with a public DNS provider. In this guide we will use the Fully Qualified Domain Name of {public-fqdn}
====

Follow the instructions at the link:{jupyterhub-installation-doc}[{jupyterhub} installation documentation] to install {jupyterhub} on {rke2} using its Helm chart

[NOTE]
====
The instructions below will guide you through the process of creating a config.yaml file that will be required by the Helm installation command.
====

Create the config.yaml file that will contain the public TLS certificate and associated private key.
+
. 



// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Detail the steps of the installation procedure.
// - The reader should be able to copy and paste commands to
//   a local environment or follow along locally with screenshots.
// - Be sure tp include verifications that the reader can use to
//   validate that major steps were performed correctly.
//
// - Make use of:
//   - Ordered lists
//   - Code blocks
//   - Screenshots
//   - Admonitions
//
// - If multiple installation methods are to be detailed, then
//   - Create a summary list here.
//   - Detail each method in a subsection.
//
//   NOTE: For solutions involving SUSE Rancher, it is preferred
//         to highlight installation through the Rancher Apps Catalog
//         with appropriate screenshots and SUSE branding.
//         Alternatively or additionally, installation via the
//         command line can be documented.
//
// - Complex installations may be broken into one or more subsections.
//
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

You'll use the {rancher-full} interface to create the {rke2} cluster on {harvester}. 




== Demonstration

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Illustrate functionality of the solution for a use case.
//
// NOTE: This demonstration text could be used to script a
//       demonstration video.
//
// - Typical demonstration flow is:
//   1. Outline the demonstration.
//   2. Prepare the environment.
//      This should be minimal, such as downloading some data to use.
//   3. Perform the demonstration.
//   4. Do not overuse screenshots.
//   5. Clean up the environment.
//
// - Make use of ordered lists, code blocks, admonitions, and screenshots.
//
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =


Demonstration or validation procedures



== Summary

// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Summarize:
// - Motivation for the guide (1 sentence)
// - What was covered (1-2 sentences)
// - Next steps (unordered list of 2-4 further learning resources)
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

Summary of this guide




// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
// Do not modify below this break.
// = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

++++
<?pdfpagebreak?>
++++


:leveloffset: 0

== Legal notice
include::common_trd_legal_notice.adoc[]

++++
<?pdfpagebreak?>
++++


:leveloffset: 0
include::common_gfdl1.2_i.adoc[]

//end
